services:
  ai-toolkit:
    build:
      context: .
      dockerfile: Dockerfile.ai-toolkit.rtx3060
    container_name: ai-toolkit
    image: ai-toolkit:latest
    working_dir: /app
    ports:
      - "11000:8000"
    volumes:
      - ./config:/app/config
      - ./workspace:/app/workspace
      - ./:/workspace:rw
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - PYTHONUNBUFFERED=1
      - TOKENIZERS_PARALLELISM=false
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    stdin_open: true
    tty: true

  tabbyml:
    image: tabbyml/tabby:latest
    container_name: tabbyml
    command: ["serve", "--model", "StarCoder-1B", "--device", "cuda"]
    ports:
      - "11001:8080"
    volumes:
      - tabby-data:/data
    # Use GPU for acceleration
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

volumes:
  tabby-data:
