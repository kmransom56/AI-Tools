services:
  ai-toolkit:
    build:
      context: .
      dockerfile: Dockerfile.ai-toolkit.rtx3060
    container_name: ai-toolkit
    image: ai-toolkit:latest
    working_dir: /app
    ports:
      - "11000:8000"
    volumes:
      - ./config:/app/config
      - ./workspace:/app/workspace
      - ./:/workspace:rw
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - POWERINFER_HOST=${POWERINFER_HOST}
      - POWERINFER_MODEL=${POWERINFER_MODEL}
      - PYTHONUNBUFFERED=1
      - TOKENIZERS_PARALLELISM=false
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
      # MCP Discovery Service integration
      - MCP_DISCOVERY_URL=http://mcp-discovery:5000
      - MCP_DISCOVERY_TIMEOUT=10.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    stdin_open: true
    tty: true

  tabbyml:
    image: tabbyml/tabby:latest
    container_name: tabbyml
    command: ["serve", "--model", "StarCoder-1B", "--device", "cuda"]
    ports:
      - "11001:8080"
    volumes:
      - tabby-data:/data
    # Use GPU for acceleration
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  # -------------------------------------------------------------------
  # cagent â€“ AI agent runtime with tool integration
  cagent:
    image: cagent:latest
    container_name: cagent
    command: ["serve"]
    ports:
      - "11002:8000"
    volumes:
      - ./cagent_examples:/app/examples
      - ./cagent_examples/tool_catalog.yaml:/app/tool_catalog.yaml
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    restart: unless-stopped
    stdin_open: true
    tty: true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s
  mcp-server:
    image: ghcr.io/github/github-mcp-server:latest
    container_name: mcp-server
    ports:
      - "11003:8000"
    environment:
      - MCP_API_KEY=${MCP_API_KEY}
      - GITHUB_TOKEN=${GITHUB_TOKEN}
    restart: unless-stopped
    healthcheck:
      test: ["CMD","curl","-f","http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s
    labels:
      mcp.type: github
      mcp.name: github-mcp
      mcp.endpoint: http://mcp-server:8000
      mcp.capabilities: "repository,issues,pull_requests"

  # -------------------------------------------------------------------
  # MCP Discovery Service - Dynamic MCP server discovery
  # https://github.com/kmransom56/mcp-discovery
  mcp-discovery-redis:
    image: redis:7-alpine
    container_name: mcp-discovery-redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --appendfsync everysec
    volumes:
      - mcp-redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  mcp-discovery:
    image: kmransom56/mcp-discovery:latest
    build:
      context: ../mcp-discovery
      dockerfile: Dockerfile
    container_name: mcp-discovery
    ports:
      - "5000:5000"
    environment:
      - HOST=0.0.0.0
      - PORT=5000
      - DEBUG=false
      - REDIS_HOST=mcp-discovery-redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - DOCKER_HOST=unix:///var/run/docker.sock
      - DISCOVERY_INTERVAL=30
      - SERVICE_TTL=60
      - ENABLE_AUTO_DISCOVERY=true
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      mcp-discovery-redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    labels:
      mcp.type: discovery
      mcp.name: mcp-discovery
      mcp.endpoint: http://mcp-discovery:5000
      mcp.capabilities: "discovery,registry,cache"

  # -------------------------------------------------------------------
  # Open WebUI - User-friendly AI Interface with GPU acceleration
  # https://github.com/open-webui/open-webui
  open-webui:
    image: ghcr.io/open-webui/open-webui:cuda
    container_name: open-webui
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-}
      - ENV=prod
      - PORT=8080
      - USE_CUDA_DOCKER=true
      - USE_CUDA_DOCKER_VER=cu128
      - OLLAMA_BASE_URL=/ollama
      - OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1
      - ANONYMIZED_TELEMETRY=false
      - DO_NOT_TRACK=true
      - SCARF_NO_ANALYTICS=true
      - WHISPER_MODEL=base
      - RAG_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - SENTENCE_TRANSFORMERS_HOME=/app/backend/data/cache/embedding/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # -------------------------------------------------------------------
  # PowerInfer - High-speed Local LLM Inference Engine
  # https://github.com/SJTU-IPADS/PowerInfer
  powerinfer:
    build:
      context: .
      dockerfile: Dockerfile.powerinfer
    container_name: powerinfer
    ports:
      - "8081:8080"
    volumes:
      - ./PowerInfer/models:/models
    environment:
      - MODEL_PATH=/models/bamboo-7b-dpo-v0.1.q4.powerinfer.gguf
      - VRAM_BUDGET=10
      - THREADS=8
      - CONTEXT_SIZE=2048
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      /app/build/bin/server
      -m /models/bamboo-7b-dpo-v0.1.q4.powerinfer.gguf
      --host 0.0.0.0
      --port 8080
      --vram-budget 10
      -t 8
      -c 2048
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

volumes:

  tabby-data:
  mcp-redis-data:
  open-webui:
    external: true
