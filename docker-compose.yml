services:
  ai-toolkit:
    build:
      context: .
      dockerfile: Dockerfile.ai-toolkit
    image: ai-toolkit:latest
    container_name: ai-toolkit
    working_dir: /app
    # To expose the web UI on the host, uncomment and set a mapping such as "8000:8000"
    # ports:
    #  - "8000:8000"
    volumes:
      - ./config:/app/config
      - ./workspace:/app/workspace
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    restart: unless-stopped
    stdin_open: true
    tty: true

  # TabbyML - Self-hosted AI coding assistant
  tabbyml:
    image: tabbyml/tabby:latest
    container_name: tabbyml
    command: ["serve", "--model", "StarCoder-1B", "--device", "cpu", "--chat-model", "Qwen2-1.5B-Instruct"]
    # If you want to expose TabbyML on a host port, set a mapping (example: 3000:3000)
    # ports:
    #  - "3000:3000"
    volumes:
      - tabby-data:/data
    restart: unless-stopped

  # cagent - Docker's cagent for container metadata/metrics (inline)
  cagent:
    image: docker/cagent:latest
    container_name: cagent
    environment:
      - CAGENT_BACKEND_URL=${CAGENT_BACKEND_URL}
      - CAGENT_TOKEN=${CAGENT_TOKEN}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./cagent/config:/etc/cagent:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3

volumes:
  tabby-data:
