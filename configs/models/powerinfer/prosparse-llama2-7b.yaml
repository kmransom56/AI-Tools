# PowerInfer Model Policy: ProSparse Llama2-7B
# Fastest option with 90% sparsity - optimized for speed

engine: powerinfer
model_path: PowerInfer/models/prosparse-llama-2-7b.q4_0.gguf

generation:
  max_tokens: 512
  temperature: 0.7
  top_p: 0.9
  top_k: 40

runtime:
  threads: 8
  gpu_layers: 28
  vram_budget_gb: 8
  batch_size: 4
  context_size: 2048
  timeout: 300

audit:
  model_version: "prosparse-llama2-7b-q4"
  source_repo: "hf://SparseLLM/prosparse-llama-2-7b"
  sparsity: "90%"
  checksum: ""
  created_at: "2026-01-18"

metadata:
  description: "ProSparse Llama2-7B Q4 - Fastest with 90% sparsity"
  use_cases:
    - "Fast responses"
    - "High-volume requests"
    - "Real-time applications"
  performance:
    tokens_per_second: "20-25"
    vram_usage: "5-7GB"
    model_size: "~3.5GB"
  hardware:
    recommended_gpu: "RTX 3060 or better"
    minimum_vram: "6GB"
    recommended_ram: "12GB"
