services:
  ai-toolkit:
    build:
      context: .
      # Switch to Dockerfile.ai-toolkit.k80 for Tesla K80 GPUs
      dockerfile: Dockerfile.ai-toolkit.rtx3060
    image: ai-toolkit:latest
    container_name: ai-toolkit
    working_dir: /app
    # To expose the web UI on the host, uncomment and set a mapping such as "8000:8000" or use the port manager
    ports:
      - "11000:8000"
    volumes:
      - ./config:/app/config
      - ./workspace:/app/workspace
      # Mount repo root to /workspace so cagent examples are accessible
      - ./:/workspace:rw
      # Mount docker socket so the container can create other containers for cagent
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      # For GPU support on WSL2, ensure NVIDIA Container Toolkit is installed:
      # https://docs.nvidia.com/cuda/wsl-user-guide/index.html
    restart: unless-stopped
    stdin_open: true
    tty: true

  # TabbyML - Self-hosted AI coding assistant
  tabbyml:
    image: tabbyml/tabby:latest
    container_name: tabbyml
    # Use CPU mode by default (GPU support requires NVIDIA Container Toolkit on WSL2)
    command: ["serve", "--model", "StarCoder-1B", "--device", "cpu"]
    # Expose TabbyML on host port 11001 (registered via Port Manager)
    ports:
      - "11001:8080"
    volumes:
      - tabby-data:/data
    environment:
      CUDA_VISIBLE_DEVICES: ""
    restart: unless-stopped

volumes:
  tabby-data:
